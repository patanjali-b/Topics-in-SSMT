{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bwf281iwmf4T",
        "outputId": "f987d64d-8f1b-45c2-d6ef-b290dee64eea"
      },
      "id": "Bwf281iwmf4T",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "715b8157",
      "metadata": {
        "id": "715b8157"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import sentencepiece as spm\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "b8ffd1e9",
      "metadata": {
        "id": "b8ffd1e9"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Parameters\n",
        "BATCH_SIZE = 64\n",
        "EMBED_SIZE = 256\n",
        "HIDDEN_SIZE = 512\n",
        "NUM_LAYERS = 1\n",
        "LR = 0.001\n",
        "VOCAB_SIZE = 8000       # subword vocab size for each language\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "3a64be7d",
      "metadata": {
        "id": "3a64be7d"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 1. Load corpus\n",
        "data = pd.read_csv('sentences.csv')  # columns: 'src','tgt', optional prosody cols\n",
        "\n",
        "# 2. Prepare and train SentencePiece models (once)\n",
        "#    Extract columns to plain text files for reliability, then train.\n",
        "with open('src.txt', 'w', encoding='utf-8') as f_src, open('tgt.txt', 'w', encoding='utf-8') as f_tgt:\n",
        "    for s, t in zip(data['src'], data['tgt']):\n",
        "        f_src.write(s.replace('\\n',' ').strip() + '\\n')\n",
        "        f_tgt.write(t.replace('\\n',' ').strip() + '\\n')\n",
        "\n",
        "\n",
        "#    Train SentencePiece on the extracted files\n",
        "#    This generates src_spm.model / src_spm.vocab and tgt_spm.model / tgt_spm.vocab\n",
        "spm.SentencePieceTrainer.Train(\n",
        "    f\"--input=src.txt --model_prefix=src_spm --vocab_size={VOCAB_SIZE} \"\n",
        "    \"--model_type=bpe --unk_id=0 --pad_id=1 --bos_id=2 --eos_id=3\"\n",
        ")\n",
        "spm.SentencePieceTrainer.Train(\n",
        "    f\"--input=tgt.txt --model_prefix=tgt_spm --vocab_size={VOCAB_SIZE} \"\n",
        "    \"--model_type=bpe --unk_id=0 --pad_id=1 --bos_id=2 --eos_id=3\"\n",
        ")\n",
        "\n",
        "# 3. Load SP models\n",
        "src_sp = spm.SentencePieceProcessor(model_file='src_spm.model')\n",
        "tgt_sp = spm.SentencePieceProcessor(model_file='tgt_spm.model')\n",
        "SRC_VOCAB = src_sp.get_piece_size()\n",
        "TGT_VOCAB = tgt_sp.get_piece_size()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "616d68e4",
      "metadata": {
        "id": "616d68e4"
      },
      "outputs": [],
      "source": [
        "def add_special(ids, sos_id, eos_id):\n",
        "    return [sos_id] + ids + [eos_id]\n",
        "\n",
        "class MTDataset(Dataset):\n",
        "    def __init__(self, df, src_sp, tgt_sp, prosody_cols=None):\n",
        "        self.src = df['src'].tolist()\n",
        "        self.tgt = df['tgt'].tolist()\n",
        "        self.prosody_cols = prosody_cols\n",
        "        if prosody_cols:\n",
        "            self.prosody = df[prosody_cols].values.astype(float)\n",
        "        else:\n",
        "            self.prosody = None\n",
        "        self.src_sp = src_sp\n",
        "        self.tgt_sp = tgt_sp\n",
        "        # special IDs\n",
        "        self.src_sos, self.src_eos = src_sp.bos_id(), src_sp.eos_id()\n",
        "        self.tgt_sos, self.tgt_eos = tgt_sp.bos_id(), tgt_sp.eos_id()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src)\n",
        "\n",
        "    def encode(self, text, sp):\n",
        "        return sp.encode(text, out_type=int)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_ids = add_special(self.encode(self.src[idx], self.src_sp), self.src_sos, self.src_eos)\n",
        "        tgt_ids = add_special(self.encode(self.tgt[idx], self.tgt_sp), self.tgt_sos, self.tgt_eos)\n",
        "        pros = self.prosody[idx] if self.prosody is not None else None\n",
        "        return torch.tensor(src_ids), torch.tensor(tgt_ids), pros\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch, pros_batch = zip(*batch)\n",
        "    src_pad = pad_sequence(src_batch, padding_value=src_sp.pad_id(), batch_first=True)\n",
        "    tgt_pad = pad_sequence(tgt_batch, padding_value=tgt_sp.pad_id(), batch_first=True)\n",
        "    pros = torch.tensor(pros_batch, dtype=torch.float) if pros_batch[0] is not None else None\n",
        "    return src_pad.to(DEVICE), tgt_pad.to(DEVICE), pros\n",
        "\n",
        "\n",
        "\n",
        "# Split into train/test\n",
        "dataset = MTDataset(data, src_sp, tgt_sp, prosody_cols=None)\n",
        "train_size = int(0.98 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_data, test_data = random_split(dataset, [train_size, test_size])\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_data, batch_size=1, shuffle=True, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print 5 samples from the training set\n",
        "print(\"Training Set Samples:\")\n",
        "for i, (src, tgt, _) in enumerate(train_loader):\n",
        "    if i >= 5:\n",
        "        break\n",
        "    print(f\"Sample {i+1}:\")\n",
        "    print(f\"Source: {src_sp.decode(src[0].tolist())}\")\n",
        "    print(f\"Target: {tgt_sp.decode(tgt[0].tolist())}\")\n",
        "    print(\"---\")\n",
        "\n",
        "# Print 5 samples from the testing set\n",
        "print(\"\\nTesting Set Samples:\")\n",
        "for i, (src, tgt, _) in enumerate(test_loader):\n",
        "    if i >= 5:\n",
        "        break\n",
        "    print(f\"Sample {i+1}:\")\n",
        "    print(f\"Source: {src_sp.decode(src[0].tolist())}\")\n",
        "    print(f\"Target: {tgt_sp.decode(tgt[0].tolist())}\")\n",
        "    print(\"---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZH6zZSYt9rD",
        "outputId": "9976760b-a680-4174-d6f6-cb2e50a6e174"
      },
      "id": "tZH6zZSYt9rD",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set Samples:\n",
            "Sample 1:\n",
            "Source: The report of the Committee of the House of Commons painted so black a picture of Newgate as then conducted, that the Corporation were roused in very shame\n",
            "Target: హౌస్ ఆఫ్ కామన్స్ కమిటీ నివేదిక న్యూగేట్ యొక్క చిత్రాన్ని చాలా నల్లగా చిత్రీకరించింది, కార్పొరేషన్ చాలా అవమానానికి గురైంది\n",
            "---\n",
            "Sample 2:\n",
            "Source: but his sporting operations did not prosper, and he became a needy man, always driven to desperate straits for cash.\n",
            "Target: కానీ అతని క్రీడా కార్యకలాపాలు అభివృద్ధి చెందలేదు మరియు అతను నిరుపేద వ్యక్తి అయ్యాడు, ఎల్లప్పుడూ నగదు కోసం తీరని కష్టాలకు నెట్టబడ్డాడు.\n",
            "---\n",
            "Sample 3:\n",
            "Source: of more depraved and systematic criminals.\n",
            "Target: మరింత చెడిపోయిన మరియు క్రమబద్ధమైన నేరస్థులు.\n",
            "---\n",
            "Sample 4:\n",
            "Source: This was the question which presented itself to the fertile brain of one Pierce,\n",
            "Target: ఇది ఒక పియర్స్ యొక్క సారవంతమైన మెదడుకు అందించిన ప్రశ్న,\n",
            "---\n",
            "Sample 5:\n",
            "Source: Few of the Newgate notorieties of late years show any marked peculiarities;\n",
            "Target: చివరి సంవత్సరాలలో న్యూగేట్ అపఖ్యాతి పాలైన వాటిలో కొన్ని గుర్తించదగిన ప్రత్యేకతలను చూపుతాయి;\n",
            "---\n",
            "\n",
            "Testing Set Samples:\n",
            "Sample 1:\n",
            "Source: was the moving spirit among these commissioners, and he is now generally recognized as the originator of modern prison architecture.\n",
            "Target: ఈ కమీషనర్లలో కదిలే స్పిరిట్, మరియు అతను ఇప్పుడు సాధారణంగా ఆధునిక జైలు నిర్మాణ రూపకర్తగా గుర్తించబడ్డాడు.\n",
            "---\n",
            "Sample 2:\n",
            "Source: The sufferer was a porter on the London, Chatham, and Dover railway, sentenced to death for shooting the station-master at Dover.\n",
            "Target: బాధితుడు లండన్, చాతం మరియు డోవర్ రైల్వేలో పోర్టర్, డోవర్ వద్ద స్టేషన్-మాస్టర్ ను కాల్చి చంపినందుకు మరణశిక్ష విధించబడింది.\n",
            "---\n",
            "Sample 3:\n",
            "Source: and Susannah Evans, in October the same year, for two shillings, with costs of six shillings, eight pence.\n",
            "Target: మరియు సుసన్నా ఎవాన్స్, అదే సంవత్సరం అక్టోబర్ లో, రెండు షిల్లింగ్ లకు, ఆరు షిల్లింగ్ లు, ఎనిమిది పెన్స్ లు.\n",
            "---\n",
            "Sample 4:\n",
            "Source: are dazzling and unpleasant to the eye owing to the clumsy thickening and vulgar thinning of the lines:\n",
            "Target: పంక్తులు వికృతంగా గట్టిపడటం మరియు అసభ్యంగా సన్నబడటం వలన కంటికి మిరుమిట్లు మరియు అసహ్యకరమైనవి:\n",
            "---\n",
            "Sample 5:\n",
            "Source: Cards were issued, and to such an extent, that although the service was not to commence till half-past ten, by nine a.m.\n",
            "Target: కార్డులు జారీ చేయబడ్డాయి మరియు ఆ మేరకు, సేవ పదిన్నర వరకు ప్రారంభం కానప్పటికీ, ఉదయం తొమ్మిది గంటల వరకు\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "5268e47d",
      "metadata": {
        "id": "5268e47d"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 5. Encoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
        "        self.gru = nn.GRU(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        outputs, hidden = self.gru(embedded)\n",
        "        return outputs, hidden\n",
        "\n",
        "# 6. Decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
        "        self.gru = nn.GRU(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc_out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = x.unsqueeze(1)\n",
        "        embedded = self.embedding(x)\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "        pred = self.fc_out(output.squeeze(1))\n",
        "        return pred, hidden\n",
        "\n",
        "# 7. Seq2Seq\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
        "        batch_size, tgt_len = tgt.size()\n",
        "        outputs = torch.zeros(batch_size, tgt_len, TGT_VOCAB).to(DEVICE)\n",
        "        _, hidden = self.encoder(src)\n",
        "        input = tgt[:, 0]\n",
        "        for t in range(1, tgt_len):\n",
        "            out, hidden = self.decoder(input, hidden)\n",
        "            outputs[:, t, :] = out\n",
        "            input = tgt[:, t] if torch.rand(1).item() < teacher_forcing_ratio else out.argmax(1)\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_bleu(model, dataloader):\n",
        "    model.eval()\n",
        "    refs, hyps = [], []\n",
        "\n",
        "    # print(\"Sample translations:\\n\")\n",
        "    with torch.no_grad():\n",
        "        for i, (src, tgt, _) in enumerate(dataloader):\n",
        "            _, hidden = model.encoder(src)\n",
        "            input = torch.tensor([tgt_sp.bos_id()] * src.size(0)).to(DEVICE)\n",
        "            outputs = []\n",
        "            for _ in range(50):\n",
        "                out, hidden = model.decoder(input, hidden)\n",
        "                input = out.argmax(1)\n",
        "                outputs.append(input)\n",
        "            outputs = torch.stack(outputs, dim=1).cpu().tolist()\n",
        "            tgt = tgt.cpu().tolist()\n",
        "            for ref, hyp in zip(tgt, outputs):\n",
        "                ref_tokens = [t for t in ref[1:] if t not in [tgt_sp.pad_id(), tgt_sp.eos_id()]]\n",
        "                hyp_tokens = [t for t in hyp if t not in [tgt_sp.pad_id(), tgt_sp.eos_id()]]\n",
        "                refs.append([ref_tokens])\n",
        "                hyps.append(hyp_tokens)\n",
        "                if i < 5:\n",
        "                    ref_text = tgt_sp.decode(ref_tokens)\n",
        "                    hyp_text = tgt_sp.decode(hyp_tokens)\n",
        "    bleu = corpus_bleu(refs, hyps, smoothing_function=SmoothingFunction().method4)\n",
        "    print(f\"BLEU Score: {bleu:.4f}\")"
      ],
      "metadata": {
        "id": "fPFUILT4op3v"
      },
      "id": "fPFUILT4op3v",
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "a9bdd40f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9bdd40f",
        "outputId": "f226aaba-d80e-4436-d9b0-52c4a7429cc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 7.5393\n",
            "Epoch 2, Loss: 7.1086\n",
            "Epoch 3, Loss: 6.8686\n",
            "Epoch 4, Loss: 6.6067\n",
            "Epoch 5, Loss: 6.3074\n",
            "Epoch 6, Loss: 6.0260\n",
            "Epoch 7, Loss: 5.7120\n",
            "Epoch 8, Loss: 5.4271\n",
            "Epoch 9, Loss: 5.1301\n",
            "Epoch 10, Loss: 4.8723\n",
            "Epoch 11, Loss: 4.5836\n",
            "Epoch 12, Loss: 4.2979\n",
            "Epoch 13, Loss: 4.1173\n",
            "Epoch 14, Loss: 3.7761\n",
            "Epoch 15, Loss: 3.5722\n",
            "Epoch 16, Loss: 3.2240\n",
            "Epoch 17, Loss: 2.9612\n",
            "Epoch 18, Loss: 2.7236\n",
            "Epoch 19, Loss: 2.3499\n",
            "Epoch 20, Loss: 2.0534\n",
            "Epoch 21, Loss: 1.8165\n",
            "Epoch 22, Loss: 1.5074\n",
            "Epoch 23, Loss: 1.2438\n",
            "Epoch 24, Loss: 0.9970\n",
            "Epoch 25, Loss: 0.7740\n",
            "Epoch 26, Loss: 0.5935\n",
            "Epoch 27, Loss: 0.4263\n",
            "Epoch 28, Loss: 0.3003\n",
            "Epoch 29, Loss: 0.2148\n",
            "Epoch 30, Loss: 0.1529\n"
          ]
        }
      ],
      "source": [
        "def train(model, dataloader, optimizer, criterion, epochs=30):\n",
        "    model.to(DEVICE)\n",
        "    for ep in range(epochs):\n",
        "        model.train()\n",
        "        total = 0\n",
        "        for src_batch, tgt_batch, pros in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(src_batch, tgt_batch)\n",
        "            out = preds[:, 1:].reshape(-1, preds.size(-1))\n",
        "            tgt = tgt_batch[:, 1:].reshape(-1)\n",
        "            loss = criterion(out, tgt)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total += loss.item()\n",
        "        print(f\"Epoch {ep+1}, Loss: {total/len(dataloader):.4f}\")\n",
        "\n",
        "enc = Encoder(SRC_VOCAB, EMBED_SIZE, HIDDEN_SIZE, NUM_LAYERS)\n",
        "# attn = Attention(HIDDEN_SIZE)\n",
        "dec = Decoder(TGT_VOCAB, EMBED_SIZE, HIDDEN_SIZE, NUM_LAYERS)\n",
        "model = Seq2Seq(enc, dec)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=src_sp.pad_id())\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train(model, train_loader, optimizer, criterion)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_bleu(model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZ5gh9th0ttn",
        "outputId": "75dffd6f-9c81-4b0b-b081-eb89fa83de58"
      },
      "id": "zZ5gh9th0ttn",
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score: 0.2056\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}