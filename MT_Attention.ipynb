{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bwf281iwmf4T",
        "outputId": "32443be4-e7c5-40d1-e732-82f30e06fa20"
      },
      "id": "Bwf281iwmf4T",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# English\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
        "!gunzip cc.en.300.vec.gz\n",
        "\n",
        "# Telugu\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.te.300.vec.gz\n",
        "!gunzip cc.te.300.vec.gz\n"
      ],
      "metadata": {
        "id": "l2bPt8Z57lhl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "addd2481-ac27-484c-f43b-9254f1cb1a2b"
      },
      "id": "l2bPt8Z57lhl",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-05 19:45:34--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.171.198.46, 3.171.198.102, 3.171.198.8, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.171.198.46|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1325960915 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘cc.en.300.vec.gz.1’\n",
            "\n",
            "cc.en.300.vec.gz.1  100%[===================>]   1.23G   260MB/s    in 4.8s    \n",
            "\n",
            "2025-05-05 19:45:39 (262 MB/s) - ‘cc.en.300.vec.gz.1’ saved [1325960915/1325960915]\n",
            "\n",
            "--2025-05-05 19:46:30--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.te.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.165.75.91, 3.165.75.95, 3.165.75.59, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.165.75.91|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1117025794 (1.0G) [binary/octet-stream]\n",
            "Saving to: ‘cc.te.300.vec.gz.1’\n",
            "\n",
            "cc.te.300.vec.gz.1  100%[===================>]   1.04G   147MB/s    in 5.7s    \n",
            "\n",
            "2025-05-05 19:46:36 (186 MB/s) - ‘cc.te.300.vec.gz.1’ saved [1117025794/1117025794]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "715b8157",
      "metadata": {
        "id": "715b8157"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import sentencepiece as spm\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "import numpy as np\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b8ffd1e9",
      "metadata": {
        "id": "b8ffd1e9"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "EMBED_SIZE = 300  # updated for FastText\n",
        "HIDDEN_SIZE = 512\n",
        "NUM_LAYERS = 1\n",
        "LR = 0.001\n",
        "VOCAB_SIZE = 8000\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load pretrained embeddings\n",
        "\n",
        "def load_pretrained_embeddings(sp_model, embedding_path, embedding_dim):\n",
        "    embeddings_index = {}\n",
        "    with open(embedding_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        next(f)\n",
        "        for line in f:\n",
        "            values = line.rstrip().split(' ')\n",
        "            word = values[0]\n",
        "            vec = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = vec\n",
        "\n",
        "    vocab_size = sp_model.get_piece_size()\n",
        "    embedding_matrix = np.random.normal(0, 1, (vocab_size, embedding_dim)).astype('float32')\n",
        "\n",
        "    for i in range(vocab_size):\n",
        "        token = sp_model.id_to_piece(i)\n",
        "        clean_token = token[1:] if token.startswith('▁') else token\n",
        "        if clean_token in embeddings_index:\n",
        "            embedding_matrix[i] = embeddings_index[clean_token]\n",
        "    return torch.tensor(embedding_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "3a64be7d",
      "metadata": {
        "id": "3a64be7d"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 1. Load corpus\n",
        "data = pd.read_csv('sentences.csv')  # columns: 'src','tgt', optional prosody cols\n",
        "\n",
        "# 2. Prepare and train SentencePiece models (once)\n",
        "#    Extract columns to plain text files for reliability, then train.\n",
        "with open('src.txt', 'w', encoding='utf-8') as f_src, open('tgt.txt', 'w', encoding='utf-8') as f_tgt:\n",
        "    for s, t in zip(data['src'], data['tgt']):\n",
        "        f_src.write(s.replace('\\n',' ').strip() + '\\n')\n",
        "        f_tgt.write(t.replace('\\n',' ').strip() + '\\n')\n",
        "\n",
        "\n",
        "#    Train SentencePiece on the extracted files\n",
        "#    This generates src_spm.model / src_spm.vocab and tgt_spm.model / tgt_spm.vocab\n",
        "spm.SentencePieceTrainer.Train(\n",
        "    f\"--input=src.txt --model_prefix=src_spm --vocab_size={VOCAB_SIZE} \"\n",
        "    \"--model_type=bpe --unk_id=0 --pad_id=1 --bos_id=2 --eos_id=3\"\n",
        ")\n",
        "spm.SentencePieceTrainer.Train(\n",
        "    f\"--input=tgt.txt --model_prefix=tgt_spm --vocab_size={VOCAB_SIZE} \"\n",
        "    \"--model_type=bpe --unk_id=0 --pad_id=1 --bos_id=2 --eos_id=3\"\n",
        ")\n",
        "\n",
        "# 3. Load SP models\n",
        "src_sp = spm.SentencePieceProcessor(model_file='src_spm.model')\n",
        "tgt_sp = spm.SentencePieceProcessor(model_file='tgt_spm.model')\n",
        "SRC_VOCAB = src_sp.get_piece_size()\n",
        "TGT_VOCAB = tgt_sp.get_piece_size()\n",
        "\n",
        "\n",
        "src_embed_weights = load_pretrained_embeddings(src_sp, \"cc.en.300.vec\", EMBED_SIZE)\n",
        "tgt_embed_weights = load_pretrained_embeddings(tgt_sp, \"cc.te.300.vec\", EMBED_SIZE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "616d68e4",
      "metadata": {
        "id": "616d68e4"
      },
      "outputs": [],
      "source": [
        "def add_special(ids, sos_id, eos_id):\n",
        "    return [sos_id] + ids + [eos_id]\n",
        "\n",
        "class MTDataset(Dataset):\n",
        "    def __init__(self, df, src_sp, tgt_sp, prosody_cols=None):\n",
        "        self.src = df['src'].tolist()\n",
        "        self.tgt = df['tgt'].tolist()\n",
        "        self.prosody_cols = prosody_cols\n",
        "        if prosody_cols:\n",
        "            self.prosody = df[prosody_cols].values.astype(float)\n",
        "        else:\n",
        "            self.prosody = None\n",
        "        self.src_sp = src_sp\n",
        "        self.tgt_sp = tgt_sp\n",
        "        # special IDs\n",
        "        self.src_sos, self.src_eos = src_sp.bos_id(), src_sp.eos_id()\n",
        "        self.tgt_sos, self.tgt_eos = tgt_sp.bos_id(), tgt_sp.eos_id()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src)\n",
        "\n",
        "    def encode(self, text, sp):\n",
        "        return sp.encode(text, out_type=int)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_ids = add_special(self.encode(self.src[idx], self.src_sp), self.src_sos, self.src_eos)\n",
        "        tgt_ids = add_special(self.encode(self.tgt[idx], self.tgt_sp), self.tgt_sos, self.tgt_eos)\n",
        "        pros = self.prosody[idx] if self.prosody is not None else None\n",
        "        return torch.tensor(src_ids), torch.tensor(tgt_ids), pros\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch, pros_batch = zip(*batch)\n",
        "    src_pad = pad_sequence(src_batch, padding_value=src_sp.pad_id(), batch_first=True)\n",
        "    tgt_pad = pad_sequence(tgt_batch, padding_value=tgt_sp.pad_id(), batch_first=True)\n",
        "    pros = torch.tensor(pros_batch, dtype=torch.float) if pros_batch[0] is not None else None\n",
        "    return src_pad.to(DEVICE), tgt_pad.to(DEVICE), pros\n",
        "\n",
        "\n",
        "\n",
        "# Split into train/test\n",
        "dataset = MTDataset(data, src_sp, tgt_sp, prosody_cols=None)\n",
        "train_size = int(0.98 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_data, test_data = random_split(dataset, [train_size, test_size])\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_data, batch_size=1, shuffle=True, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print 5 samples from the training set\n",
        "print(\"Training Set Samples:\")\n",
        "for i, (src, tgt, _) in enumerate(train_loader):\n",
        "    if i >= 5:\n",
        "        break\n",
        "    print(f\"Sample {i+1}:\")\n",
        "    print(f\"Source: {src_sp.decode(src[0].tolist())}\")\n",
        "    print(f\"Target: {tgt_sp.decode(tgt[0].tolist())}\")\n",
        "    print(\"---\")\n",
        "\n",
        "# Print 5 samples from the testing set\n",
        "print(\"\\nTesting Set Samples:\")\n",
        "for i, (src, tgt, _) in enumerate(test_loader):\n",
        "    if i >= 5:\n",
        "        break\n",
        "    print(f\"Sample {i+1}:\")\n",
        "    print(f\"Source: {src_sp.decode(src[0].tolist())}\")\n",
        "    print(f\"Target: {tgt_sp.decode(tgt[0].tolist())}\")\n",
        "    print(\"---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZH6zZSYt9rD",
        "outputId": "761b7221-ec74-4228-9da4-db9ded9540e9"
      },
      "id": "tZH6zZSYt9rD",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set Samples:\n",
            "Sample 1:\n",
            "Source: While they were in a state of insensibility the murder was committed.\n",
            "Target: వారు అపస్మారక స్థితిలో ఉండగానే ఈ హత్య జరిగింది.\n",
            "---\n",
            "Sample 2:\n",
            "Source: The Fleet, which stood in Farringdon Street,\n",
            "Target: ఫారింగ్ డన్ స్ట్రీట్ లో ఉన్న ఫ్లీట్,\n",
            "---\n",
            "Sample 3:\n",
            "Source: While thus engaged, Howard thrust the poker into the fire.\n",
            "Target: ఈ విధంగా నిశ్చితార్థం చేస్తున్నప్పుడు, హోవార్డ్ పేకాటను మంటల్లోకి నెట్టాడు.\n",
            "---\n",
            "Sample 4:\n",
            "Source: It should be peremptorily forbidden to the keeper or any officer to make a pecuniary profit out of the supplies of food, fuel, or other necessaries.\n",
            "Target: ఆహారం, ఇంధనం లేదా ఇతర అవసరాల సామాగ్రి నుండి డబ్బు లాభం పొందడం కీపర్ లేదా ఏ అధికారికి ఖచ్చితంగా నిషేధించబడాలి.\n",
            "---\n",
            "Sample 5:\n",
            "Source: convicted of obtaining jewelery under the false pretense of making silly women \"beautiful for ever.\"\n",
            "Target: వెర్రి స్త్రీలను \"ఎప్పటికీ అందంగా\" తీర్చిదిద్దుతాననే తప్పుడు నెపంతో నగలు పొందినందుకు దోషిగా నిర్ధారించబడింది.\n",
            "---\n",
            "\n",
            "Testing Set Samples:\n",
            "Sample 1:\n",
            "Source: He was an honest sea-captain, he said, trading from Liverpool, where once he had the misfortune to be introduced to a man of wealth,\n",
            "Target: అతను నిజాయితీపరుడైన సీ-కెప్టెన్ అని, అతను లివర్ పూల్ నుండి వర్తకం చేస్తున్నాడని, ఒకప్పుడు సంపద కలిగిన వ్యక్తితో పరిచయం కావాల్సిన దురదృష్టం ఎదురైంది.\n",
            "---\n",
            "Sample 2:\n",
            "Source: His hair, though gray, was thick, and lay smooth over his forehead.\n",
            "Target: అతని జుట్టు, బూడిద రంగులో ఉన్నప్పటికీ, మందంగా ఉంది మరియు అతని నుదిటిపై మృదువైనది.\n",
            "---\n",
            "Sample 3:\n",
            "Source: On that day Oxford was on the watch at Buckingham Palace.\n",
            "Target: ఆ రోజు ఆక్స్ ఫర్డ్ బకింగ్ హామ్ ప్యాలెస్ లో నిఘా ఉంచింది.\n",
            "---\n",
            "Sample 4:\n",
            "Source: \"We cannot close these remarks,\" say the inspectors, \"without an expression of the painful feelings with which we submit to your Lordship\n",
            "Target: \"మేము ఈ వ్యాఖ్యలను మూసివేయలేము,\" అని ఇన్స్పెక్టర్లు చెప్పారు, \"మేము మీ భగవంతునికి సమర్పించే బాధాకరమైన అనుభూతుల వ్యక్తీకరణ లేకుండా.\n",
            "---\n",
            "Sample 5:\n",
            "Source: For these crimes William Roupell was tried at the Central Criminal Court on the twenty-fourth September, eighteen sixty-two.\n",
            "Target: ఈ నేరాల కోసం విలియం రూపెల్ ను సెంట్రల్ క్రిమినల్ కోర్ట్ లో ఇరవై నాలుగవ సెప్టెంబర్, పద్దెనిమిది అరవై రెండు నాడు విచారించారు.\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, pretrained=None):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=1)\n",
        "        if pretrained is not None:\n",
        "            self.embedding.weight.data.copy_(pretrained)\n",
        "            self.embedding.weight.requires_grad = False\n",
        "        self.gru = nn.GRU(embed_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_size * 2, hidden_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        outputs, hidden = self.gru(embedded)\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))).unsqueeze(0)\n",
        "        return outputs, hidden\n",
        "\n",
        "# Decoder without attention\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, pretrained=None):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=1)\n",
        "        if pretrained is not None:\n",
        "            self.embedding.weight.data.copy_(pretrained)\n",
        "            self.embedding.weight.requires_grad = False\n",
        "        self.gru = nn.GRU(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc_out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = x.unsqueeze(1)\n",
        "        embedded = self.embedding(x)\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "        output = self.fc_out(output.squeeze(1))\n",
        "        return output, hidden\n",
        "\n",
        "# Seq2Seq\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
        "        batch_size, tgt_len = tgt.size()\n",
        "        outputs = torch.zeros(batch_size, tgt_len, TGT_VOCAB).to(DEVICE)\n",
        "        _, hidden = self.encoder(src)\n",
        "        input = tgt[:, 0]\n",
        "        for t in range(1, tgt_len):\n",
        "            out, hidden = self.decoder(input, hidden)\n",
        "            outputs[:, t, :] = out\n",
        "            input = tgt[:, t] if torch.rand(1).item() < teacher_forcing_ratio else out.argmax(1)\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "Yob-gQjhCl5W"
      },
      "id": "Yob-gQjhCl5W",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_bleu(model, dataloader):\n",
        "    model.eval()\n",
        "    refs, hyps = [], []\n",
        "    print(\"Sample translations:\\n\")\n",
        "    with torch.no_grad():\n",
        "        for i, (src, tgt, _) in enumerate(dataloader):\n",
        "            _, hidden = model.encoder(src)\n",
        "            input = torch.tensor([tgt_sp.bos_id()] * src.size(0)).to(DEVICE)\n",
        "            outputs = []\n",
        "            for _ in range(50):\n",
        "                out, hidden = model.decoder(input, hidden)\n",
        "                input = out.argmax(1)\n",
        "                outputs.append(input)\n",
        "            outputs = torch.stack(outputs, dim=1).cpu().tolist()\n",
        "            tgt = tgt.cpu().tolist()\n",
        "            src = src.cpu().tolist()\n",
        "            for ref, hyp, src_seq in zip(tgt, outputs, src):\n",
        "                ref_tokens = [t for t in ref[1:] if t not in [tgt_sp.pad_id(), tgt_sp.eos_id()]]\n",
        "                hyp_tokens = [t for t in hyp if t not in [tgt_sp.pad_id(), tgt_sp.eos_id()]]\n",
        "                src_tokens = [t for t in src_seq[1:] if t not in [src_sp.pad_id(), src_sp.eos_id()]]\n",
        "                refs.append([ref_tokens])\n",
        "                hyps.append(hyp_tokens)\n",
        "                if i < 5:\n",
        "                    src_text = src_sp.decode(src_tokens)\n",
        "                    ref_text = tgt_sp.decode(ref_tokens)\n",
        "                    hyp_text = tgt_sp.decode(hyp_tokens)\n",
        "                    print(f\"Source   : {src_text}\")\n",
        "                    print(f\"Reference: {ref_text}\")\n",
        "                    print(f\"Predicted: {hyp_text}\\n\")\n",
        "    bleu = corpus_bleu(refs, hyps, smoothing_function=SmoothingFunction().method4)\n",
        "    print(f\"BLEU Score: {bleu:.4f}\")"
      ],
      "metadata": {
        "id": "fPFUILT4op3v"
      },
      "id": "fPFUILT4op3v",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "a9bdd40f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9bdd40f",
        "outputId": "584e3334-524f-4a3f-80e3-38cbd5b34eb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 7.6037\n",
            "Epoch 2, Loss: 7.2617\n",
            "Epoch 3, Loss: 7.1743\n",
            "Epoch 4, Loss: 7.0952\n",
            "Epoch 5, Loss: 7.0151\n",
            "Epoch 6, Loss: 6.9422\n",
            "Epoch 7, Loss: 6.8735\n",
            "Epoch 8, Loss: 6.7922\n",
            "Epoch 9, Loss: 6.7272\n",
            "Epoch 10, Loss: 6.6401\n",
            "Epoch 11, Loss: 6.5754\n",
            "Epoch 12, Loss: 6.4946\n",
            "Epoch 13, Loss: 6.4122\n",
            "Epoch 14, Loss: 6.3331\n",
            "Epoch 15, Loss: 6.2742\n",
            "Epoch 16, Loss: 6.2498\n",
            "Epoch 17, Loss: 6.0939\n",
            "Epoch 18, Loss: 6.0349\n",
            "Epoch 19, Loss: 5.9660\n",
            "Epoch 20, Loss: 5.9026\n",
            "Sample translations:\n",
            "\n",
            "Source   : seemed to strike her mind with horror and consternation, to the exclusion of all power of recollectedness in preparation for the approaching awful moment.\n",
            "Reference: ఆసన్నమైన భయంకర క్షణానికి సన్నాహకంగా స్మరించుకునే శక్తిని మినహాయించి, ఆమె మనస్సును భయానక మరియు దిగ్భ్రాంతితో కొట్టినట్లు అనిపించింది.\n",
            "Predicted: మరియు,,,,,,,,,,,,\n",
            "\n",
            "Source   : All privacy was impossible under the circumstances.\n",
            "Reference: పరిస్థితులలో అన్ని గోప్యత అసాధ్యం.\n",
            "Predicted: మరియు,,,,,,,,,,,,\n",
            "\n",
            "Source   : the Exchequer, the Commissioners of bankruptcy and of taxes; smugglers, and a larger number sentenced for very short terms,\n",
            "Reference: ఖజానా, దివాలా మరియు పన్నుల కమిషనర్లు; స్మగ్లర్లు, మరియు ఎక్కువ సంఖ్యలో చాలా తక్కువ వ్యవధిలో శిక్ష విధించబడింది,\n",
            "Predicted: మరియు,,,,,,,,,,,,\n",
            "\n",
            "Source   : On that day Oxford was on the watch at Buckingham Palace.\n",
            "Reference: ఆ రోజు ఆక్స్ ఫర్డ్ బకింగ్ హామ్ ప్యాలెస్ లో నిఘా ఉంచింది.\n",
            "Predicted: మరియు,,,,,,,,,,,,\n",
            "\n",
            "Source   : For where these are boldly and carefully designed, and each letter is thoroughly individual in form,\n",
            "Reference: ఇవి ధైర్యంగా మరియు జాగ్రత్తగా రూపొందించబడిన చోట, మరియు ప్రతి అక్షరం పూర్తిగా వ్యక్తిగత రూపంలో ఉంటుంది,\n",
            "Predicted: మరియు,,,,,,,,,,,,\n",
            "\n",
            "BLEU Score: 0.0008\n"
          ]
        }
      ],
      "source": [
        "def train(model, dataloader, optimizer, criterion, epochs=20):\n",
        "    model.to(DEVICE)\n",
        "    for ep in range(epochs):\n",
        "        model.train()\n",
        "        total = 0\n",
        "        for src_batch, tgt_batch, pros in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(src_batch, tgt_batch)\n",
        "            out = preds[:, 1:].reshape(-1, preds.size(-1))\n",
        "            tgt = tgt_batch[:, 1:].reshape(-1)\n",
        "            loss = criterion(out, tgt)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total += loss.item()\n",
        "        print(f\"Epoch {ep+1}, Loss: {total/len(dataloader):.4f}\")\n",
        "\n",
        "# Initialize\n",
        "enc = Encoder(SRC_VOCAB, EMBED_SIZE, HIDDEN_SIZE, NUM_LAYERS, pretrained=src_embed_weights)\n",
        "# attn = Attention(HIDDEN_SIZE)\n",
        "# dec = Decoder(TGT_VOCAB, EMBED_SIZE, HIDDEN_SIZE, NUM_LAYERS, attn,  pretrained=tgt_embed_weights)\n",
        "dec = Decoder(TGT_VOCAB, EMBED_SIZE, HIDDEN_SIZE, NUM_LAYERS,  pretrained=tgt_embed_weights)\n",
        "model = Seq2Seq(enc, dec)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=src_sp.pad_id())\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train(model, train_loader, optimizer, criterion)\n",
        "    evaluate_bleu(model, test_loader)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_bleu(model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZ5gh9th0ttn",
        "outputId": "f1886366-2898-459e-e133-4b0d90a51673"
      },
      "id": "zZ5gh9th0ttn",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample translations:\n",
            "\n",
            "Source   : A desperate and deadly struggle must have taken place in the carriage, and the stain of a bloody hand marked the door.\n",
            "Reference: క్యారేజ్ లో తీరని మరియు ఘోరమైన పోరాటం జరిగి ఉండాలి మరియు రక్తపు చేతి మరక తలుపును గుర్తించింది.\n",
            "Predicted: మరియు,,,,,,,,,,,,,,,.\n",
            "\n",
            "Source   : The inquiry was most searching and complete, and the committee spoke plainly in its report.\n",
            "Reference: విచారణ చాలా శోధించబడింది మరియు పూర్తి చేయబడింది మరియు కమిటీ తన నివేదికలో స్పష్టంగా మాట్లాడింది.\n",
            "Predicted: మరియు,,,,,,,,,,,,,,,.\n",
            "\n",
            "Source   : In this way he formed the acquaintance of Watson and others, with whom he was arraigned for treasonable practices, and imprisoned.\n",
            "Reference: ఈ విధంగా అతను వాట్సన్ మరియు ఇతరులతో పరిచయాన్ని ఏర్పరచుకున్నాడు, వీరితో అతను దేశద్రోహ చర్యలకు పాల్పడ్డాడు మరియు జైలు శిక్ష అనుభవించాడు.\n",
            "Predicted: మరియు,,,,,,,,,,,,,,,.\n",
            "\n",
            "Source   : some, especially of low stature, found it difficult to remain standing, and several, although held up for some time by the men nearest them,\n",
            "Reference: కొంతమంది, ముఖ్యంగా తక్కువ పొట్టితనాన్ని కలిగి ఉన్నవారు, నిలబడి ఉండటం కష్టంగా అనిపించింది, మరియు చాలా మంది, వారికి సమీపంలోని పురుషులు కొంత సమయం పాటు పట్టుకున్నప్పటికీ,\n",
            "Predicted: మరియు,,,,,,,,,,,,,,,.\n",
            "\n",
            "Source   : He was an honest sea-captain, he said, trading from Liverpool, where once he had the misfortune to be introduced to a man of wealth,\n",
            "Reference: అతను నిజాయితీపరుడైన సీ-కెప్టెన్ అని, అతను లివర్ పూల్ నుండి వర్తకం చేస్తున్నాడని, ఒకప్పుడు సంపద కలిగిన వ్యక్తితో పరిచయం కావాల్సిన దురదృష్టం ఎదురైంది.\n",
            "Predicted: మరియు,,,,,,,,,,,,,,,.\n",
            "\n",
            "BLEU Score: 0.0010\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}